{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P5F7hoA9ViO0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mljyflores_team\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ljyflores_team/sds632/runs/1ok8mcwy\" target=\"_blank\">fanciful-microwave-5</a></strong> to <a href=\"https://wandb.ai/ljyflores_team/sds632\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import types\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# from skorch import NeuralNetRegressor\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from loss import LossComputer\n",
        "\n",
        "from transformers import AdamW, BertTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7J4z_i6dVkaC"
      },
      "outputs": [],
      "source": [
        "class DRODataset(Dataset):\n",
        "    def __init__(self, X, y, g):\n",
        "        self.X = torch.Tensor(X)\n",
        "        self.y = torch.tensor(y)\n",
        "        self.group_to_name = {i:name for (i,name) in enumerate(set(g))}\n",
        "        self.name_to_group = {name:i for (i,name) in enumerate(set(g))}\n",
        "        self.g = torch.tensor([self.name_to_group[i] for i in g])\n",
        "        self.n_groups = len(set(g))\n",
        "        self._group_array = torch.LongTensor(self.g)\n",
        "        self._group_counts = (torch.arange(self.n_groups).unsqueeze(1)==self._group_array).sum(1).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.g)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X_row = self.X[idx].float()\n",
        "        y_row = self.y[idx].float()\n",
        "        g_row = self.g[idx].float()\n",
        "        sample = {\"X\": X_row, \"y\": y_row, \"g\":g_row}\n",
        "        return sample\n",
        "  \n",
        "    def group_counts(self):\n",
        "        return torch.bincount(self.g)\n",
        "\n",
        "    def get_loader(self, train=False, reweight_groups=False, **kwargs):\n",
        "        if not train: # Validation or testing\n",
        "            assert reweight_groups is None\n",
        "            shuffle = False\n",
        "            sampler = None\n",
        "        elif not reweight_groups: # Training but not reweighting\n",
        "            shuffle = True\n",
        "            sampler = None\n",
        "        else: # Training and reweighting\n",
        "            # When the --robust flag is not set, reweighting changes the loss function\n",
        "            # from the normal ERM (average loss over each training example)\n",
        "            # to a reweighted ERM (weighted average where each (y,c) group has equal weight) .\n",
        "            # When the --robust flag is set, reweighting does not change the loss function\n",
        "            # since the minibatch is only used for mean gradient estimation for each group separately\n",
        "            group_weights = len(self)/self._group_counts\n",
        "            weights = group_weights[self._group_array]\n",
        "\n",
        "            # Replacement needs to be set to True, otherwise we'll run out of minority samples\n",
        "            sampler = WeightedRandomSampler(weights, len(self), replacement=True)\n",
        "            shuffle = False\n",
        "\n",
        "        loader = DataLoader(\n",
        "          self,\n",
        "          shuffle=shuffle,\n",
        "          sampler=sampler,\n",
        "          **kwargs)\n",
        "        return loader\n",
        "  \n",
        "    def group_str(self, group_idx):\n",
        "        self.group_to_name[group_idx]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PaH9LAw6eES6"
      },
      "outputs": [],
      "source": [
        "def run_epoch(epoch, model, optimizer, loader, loss_computer, # logger, csv_logger, \n",
        "              args, is_training, show_progress=False, \n",
        "              log_every=50, scheduler=None):\n",
        "    \"\"\"\n",
        "    scheduler is only used inside this function if model is bert.\n",
        "    \"\"\"\n",
        "\n",
        "    if is_training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    prog_bar_loader = tqdm(loader)\n",
        "\n",
        "    with torch.set_grad_enabled(is_training):\n",
        "        for batch_idx, batch in enumerate(prog_bar_loader):\n",
        "            # Unpack batch, feed through model, compute loss\n",
        "            if args['d']=='dhs':\n",
        "                x, y, g = batch['X'].to(f'cuda:{model.device_ids[0]}'), batch['y'].to(f'cuda:{model.device_ids[0]}'), batch['g'].to(f'cuda:{model.device_ids[0]}')\n",
        "                outputs = model(x)\n",
        "                loss_main = loss_computer.loss(outputs.view(-1), y, g, is_training)\n",
        "        \n",
        "            elif args['d']=='liar':\n",
        "                batch = {k: v.to(f'cuda:{model.device_ids[0]}') for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                loss_main = loss_computer.loss(outputs['logits'], \n",
        "                                            batch['labels'], \n",
        "                                            batch['labels'], \n",
        "                                            is_training)\n",
        "      \n",
        "      \n",
        "            # Backpropagate\n",
        "            if is_training:\n",
        "                optimizer.zero_grad()\n",
        "                loss_main.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if is_training and (batch_idx+1) % log_every==0:\n",
        "                loss_computer.reset_stats()\n",
        "\n",
        "            if (not is_training) or loss_computer.batch_count > 0:\n",
        "                if is_training:\n",
        "                    loss_computer.reset_stats()\n",
        "\n",
        "\n",
        "def train(model, criterion, dataset, args, epoch_offset):    \n",
        "    # model = model.to(device)\n",
        "\n",
        "    # process generalization adjustment stuff\n",
        "    adjustments = [float(c) for c in args['generalization_adjustment'].split(',')]\n",
        "    assert len(adjustments) in (1, dataset['train_data'].n_groups)\n",
        "    if len(adjustments)==1:\n",
        "        adjustments = np.array(adjustments* dataset['train_data'].n_groups)\n",
        "    else:\n",
        "        adjustments = np.array(adjustments)\n",
        "\n",
        "    # Define loss, optimizer, and LR scheduler\n",
        "    train_loss_computer = LossComputer(\n",
        "                            criterion,\n",
        "                            is_robust=args['robust'],\n",
        "                            dataset=dataset['train_data'],\n",
        "                            alpha=args['alpha'],\n",
        "                            gamma=args['gamma'],\n",
        "                            adj=adjustments,\n",
        "                            step_size=args['robust_step_size'],\n",
        "                            normalize_loss=args['use_normalized_loss'],\n",
        "                            btl=args['btl'],\n",
        "                            min_var_weight=args['minimum_variational_weight'])\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=args['lr'],\n",
        "        # momentum=0.9,\n",
        "        weight_decay=args['weight_decay'])\n",
        "    if args['scheduler']:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            'min',\n",
        "            factor=0.1,\n",
        "            patience=5,\n",
        "            threshold=0.0001,\n",
        "            min_lr=0,\n",
        "            eps=1e-08)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    # Iterate through epochs\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(epoch_offset, epoch_offset+args['n_epochs']):\n",
        "      \n",
        "        # Training Set\n",
        "        # logger.write('\\nEpoch [%d]:\\n' % epoch)\n",
        "        print(f'Epoch {epoch}')\n",
        "        print(f'Training')\n",
        "        run_epoch(\n",
        "            epoch, model, optimizer, \n",
        "            dataset['train_loader'],\n",
        "            train_loss_computer, \n",
        "            args,\n",
        "            is_training=True,\n",
        "            show_progress=args['show_progress'],\n",
        "            log_every=args['log_every'],\n",
        "            scheduler=scheduler)\n",
        "\n",
        "        # Validation Set\n",
        "        print(f'Validation')\n",
        "        val_loss_computer = LossComputer(\n",
        "            criterion,\n",
        "            is_robust=args['robust'],\n",
        "            dataset=dataset['val_data'],\n",
        "            step_size=args['robust_step_size'],\n",
        "            alpha=args['alpha'])\n",
        "        run_epoch(\n",
        "            epoch, model, optimizer, \n",
        "            dataset['val_loader'],\n",
        "            val_loss_computer, # logger, val_csv_logger, \n",
        "            args,\n",
        "            is_training=False)\n",
        "\n",
        "        # Test set; don't print to avoid peeking\n",
        "        if dataset['test_data'] is not None:\n",
        "            test_loss_computer = LossComputer(\n",
        "                criterion,\n",
        "                is_robust=args['robust'],\n",
        "                dataset=dataset['test_data'],\n",
        "                step_size=args['robust_step_size'],\n",
        "                alpha=args['alpha'])\n",
        "            run_epoch(\n",
        "                epoch, model, optimizer, \n",
        "                dataset['test_loader'],\n",
        "                test_loss_computer, # None, test_csv_logger, \n",
        "                args,\n",
        "                is_training=False)\n",
        "\n",
        "        # Inspect learning rates\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                curr_lr = param_group['lr']\n",
        "                print(f'Current lr: {curr_lr}')\n",
        "\n",
        "        if args['scheduler'] and args['model'] != 'bert':\n",
        "            if args['robust']:\n",
        "                val_loss, _ = val_loss_computer.compute_robust_loss_greedy(val_loss_computer.avg_group_loss, val_loss_computer.avg_group_loss)\n",
        "            else:\n",
        "                val_loss = val_loss_computer.avg_actual_loss\n",
        "            scheduler.step(val_loss) #scheduler step to update lr at the end of epoch\n",
        "\n",
        "        if epoch % args['save_step'] == 0:\n",
        "            if args['d'] in ['dhs']:\n",
        "                torch.save(model, os.path.join(args['log_dir'], args['d'], '%d_model.pth' % epoch))\n",
        "            else:\n",
        "                model.module.save_pretrained(os.path.join(args['log_dir'], args['d'], '%d_model' % epoch))\n",
        "\n",
        "        if args['save_last']:\n",
        "            if args['d'] in ['dhs']:\n",
        "                torch.save(model, os.path.join(args['log_dir'], args['d'], 'last_model.pth'))\n",
        "            else:\n",
        "                model.module.save_pretrained(os.path.join(args['log_dir'], args['d'], 'last_model'))\n",
        "\n",
        "        if args['save_best']:\n",
        "            if args['robust'] or args['reweight_groups']:\n",
        "                curr_val_acc = min(val_loss_computer.avg_group_acc)\n",
        "            else:\n",
        "                curr_val_acc = val_loss_computer.avg_acc\n",
        "            print(f'Current validation accuracy: {curr_val_acc}')\n",
        "            if curr_val_acc > best_val_acc:\n",
        "                best_val_acc = curr_val_acc\n",
        "                if args['d'] in ['dhs']:\n",
        "                    torch.save(model, os.path.join(args['log_dir'], args['d'], 'best_model'))\n",
        "                else:\n",
        "                    model.module.save_pretrained(os.path.join(args['log_dir'], args['d'], 'best_model'))\n",
        "                print(f'Best model saved at epoch {epoch}')\n",
        "\n",
        "        if args['automatic_adjustment']:\n",
        "            gen_gap = val_loss_computer.avg_group_loss - train_loss_computer.exp_avg_loss\n",
        "            adjustments = gen_gap * torch.sqrt(train_loss_computer.group_counts)\n",
        "            train_loss_computer.adj = adjustments\n",
        "            print('Adjustments updated\\n')\n",
        "            for group_idx in range(train_loss_computer.n_groups):\n",
        "              pass\n",
        "        #         logger.write(\n",
        "        #             f'  {train_loss_computer.get_group_name(group_idx)}:\\t'\n",
        "        #             f'adj = {train_loss_computer.adj[group_idx]:.3f}\\n')\n",
        "        # logger.write('\\n')\n",
        "        # wandb.finish()\n",
        "    \n",
        "    return {'train_loss': train_loss_computer,\n",
        "            'val_loss':   val_loss_computer,\n",
        "            'test_loss':  test_loss_computer}\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b-VsbuxNJLck"
      },
      "outputs": [],
      "source": [
        "# Args\n",
        "args = {'s':'confounder',\n",
        "        't':None,\n",
        "        'c':None,\n",
        "        'resume':False,\n",
        "        'minority_fraction':None,\n",
        "        'imbalance_ratio':None,\n",
        "        'fraction':1.0,\n",
        "        'root_dir':None,\n",
        "        'reweight_groups':False,\n",
        "        'augment_data':False,\n",
        "        'val_fraction':0.1,\n",
        "        'alpha':0.2,\n",
        "        'generalization_adjustment':\"0.0\",\n",
        "        'automatic_adjustment':False, \n",
        "        'robust_step_size':0.01,\n",
        "        'use_normalized_loss':False,\n",
        "        'btl':False,\n",
        "        'hinge':False,\n",
        "        'train_from_scratch':False,\n",
        "        'scheduler':True,\n",
        "        'weight_decay':0.001,\n",
        "        'gamma':0.1,\n",
        "        'minimum_variational_weight':0,\n",
        "        'seed':0,\n",
        "        'show_progress':False,\n",
        "        'log_dir':'logs/',\n",
        "        'log_every':50,\n",
        "        'save_step':10,\n",
        "        'save_best':False,\n",
        "        'save_last':False,\n",
        "        'clip':0.05}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZFUb2Hv8H2W"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGdRhq88Huw"
      },
      "source": [
        "### DHS Poverty Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jckYZCRMJDbN"
      },
      "outputs": [],
      "source": [
        "def classify_num(x):\n",
        "  if x<-100000:\n",
        "    return \"High Poverty\"\n",
        "  elif x<0:\n",
        "    return \"Moderate Poverty\"\n",
        "  elif x<100000:\n",
        "    return \"Moderate Wealth\"\n",
        "  else:\n",
        "    return \"High Wealth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSo7VDS91AhY"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "dhs = pd.read_csv(\"/content/drive/MyDrive/S&DS 632/group_DRO-master/dhs.csv\")\n",
        "dhs['Wealth_Class'] = dhs['Wealth Index'].apply(classify_num)\n",
        "\n",
        "# Get X, y, g\n",
        "dhs_X = dhs.drop(['Cluster number','DHSCLUST','Wealth Index','Wealth_Class'], axis=1)\n",
        "dhs_y = dhs['Wealth Index']\n",
        "dhs_g = dhs['Wealth_Class']\n",
        "\n",
        "# Impute missing training values\n",
        "imputer = KNNImputer(n_neighbors=4)\n",
        "dhs_X = imputer.fit_transform(dhs_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUjqIzwBWFrB"
      },
      "outputs": [],
      "source": [
        "dro_dataset  = DRODataset(dhs_X, dhs_y, dhs_g)\n",
        "dro_dataload = DataLoader(dro_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "# Generate data\n",
        "data = {}\n",
        "data['train_loader'] = dro_dataload # train_loader\n",
        "data['val_loader']   = dro_dataload # val_loader\n",
        "data['test_loader']  = dro_dataload # test_loader\n",
        "data['train_data']   = dro_dataset  # train_data\n",
        "data['val_data']     = dro_dataset  # val_data\n",
        "data['test_data']    = dro_dataset  # test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRcxDjTu8QhP"
      },
      "source": [
        "### LIAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eY4Ciur4nXgp"
      },
      "outputs": [],
      "source": [
        "args['d'] = 'liar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G85U5eM9bZEh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset liar (/home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e211f6107ae49d98c5a3bcfc6bd3736",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-be82b1962002fc97.arrow\n",
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-eceadc3eed73f948.arrow\n",
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-3a111b8a2d8cb711.arrow\n",
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-eb8559b88a236780.arrow\n",
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-80adcbda198326de.arrow\n",
            "Loading cached processed dataset at /home/lily/lyf6/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514/cache-3bc3edca31a7170c.arrow\n"
          ]
        }
      ],
      "source": [
        "# Get data and splits\n",
        "dataset = load_dataset('liar')\n",
        "train_data, test_data, val_data = dataset['train'], dataset['test'], dataset['validation']\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode_dict(d):\n",
        "    return tokenizer(d['statement'], truncation=True, padding='max_length')\n",
        "\n",
        "train_data = train_data.map(encode_dict, batched=True)\n",
        "test_data  = test_data.map(encode_dict, batched=True)\n",
        "val_data   = val_data.map(encode_dict, batched=True)\n",
        "\n",
        "# Rename 'label' key to 'labels'\n",
        "train_data = train_data.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "test_data  = test_data.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "val_data   = val_data.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "\n",
        "# Format the dataset\n",
        "def format_LIAR_dataset(dataset):\n",
        "    g = np.array([int(d['label']) for d in dataset])\n",
        "    dataset.n_groups = len(set(g))\n",
        "    dataset._group_array = torch.LongTensor(g)\n",
        "    dataset._group_counts = (torch.arange(dataset.n_groups).unsqueeze(1)==dataset._group_array).sum(1).float()\n",
        "    dataset.group_counts = dataset._group_counts\n",
        "    return dataset\n",
        "\n",
        "train_data = format_LIAR_dataset(train_data)\n",
        "test_data  = format_LIAR_dataset(test_data)\n",
        "val_data   = format_LIAR_dataset(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ee3yiWebDMFw"
      },
      "outputs": [],
      "source": [
        "args['batch_size'] = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HQaseZFY8Rf-"
      },
      "outputs": [],
      "source": [
        "# Fix formatting for dataloader\n",
        "weights = (len(train_data)/train_data._group_counts)[train_data._group_array]\n",
        "sampler = WeightedRandomSampler(weights, len(train_data), replacement=True)\n",
        "train_data.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, \n",
        "                                               batch_size=args['batch_size'],\n",
        "                                               shuffle=False,\n",
        "                                               sampler=sampler)\n",
        "\n",
        "test_data.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args['batch_size'])\n",
        "\n",
        "val_data.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args['batch_size'])\n",
        "\n",
        "# Generate data\n",
        "data = {}\n",
        "data['train_data']   = train_data  # train_data\n",
        "data['val_data']     = val_data    # val_data\n",
        "data['test_data']    = test_data   # test_data\n",
        "data['train_loader'] = train_dataloader # train_loader\n",
        "data['val_loader']   = val_dataloader   # val_loader\n",
        "data['test_loader']  = test_dataloader  # test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKLn0yKN64ZM"
      },
      "source": [
        "## Model and Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwkPTpTt85ZE"
      },
      "source": [
        "### MLP Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2GTkuVo88lt"
      },
      "outputs": [],
      "source": [
        "class MLP_Regression(torch.nn.Module):\n",
        "  def __init__(self, num_variables):\n",
        "    super(MLP_Regression, self).__init__()\n",
        "    self.linear1     = torch.nn.Linear(num_variables, 128)\n",
        "    self.dropout1    = torch.nn.Dropout(p=0.5)\n",
        "    self.activation1 = torch.nn.ReLU()\n",
        "    self.linear2     = torch.nn.Linear(128, 64)\n",
        "    self.dropout2    = torch.nn.Dropout(p=0.5)\n",
        "    self.activation2 = torch.nn.ReLU()\n",
        "    self.linear3     = torch.nn.Linear(64, 32)\n",
        "    self.dropout3    = torch.nn.Dropout(p=0.5)\n",
        "    # self.activation3 = torch.nn.ReLU()\n",
        "    self.linear4     = torch.nn.Linear(32, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.activation1(self.dropout1(self.linear1(x)))\n",
        "    x = self.activation2(self.dropout2(self.linear2(x)))\n",
        "    # x = self.activation3(self.dropout3(self.linear3(x)))\n",
        "    x = self.dropout3(self.linear3(x))\n",
        "    return self.linear4(x).view(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOorADCQ8-Xv"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = MLP_Regression(156)\n",
        "\n",
        "# Criterion\n",
        "criterion = torch.nn.MSELoss(reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2wACVW58-3A"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NTwOGVYY8kIN"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zkxC_YOl64As"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config_class = BertConfig\n",
        "model_class = BertForSequenceClassification\n",
        "\n",
        "config = config_class.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=6,\n",
        "    finetuning_task='liar')\n",
        "model = model_class.from_pretrained(\n",
        "    'bert-base-uncased',# '/content/drive/MyDrive/S&DS 632/logs/last_model',\n",
        "    from_tf=False,\n",
        "    config=config)\n",
        "\n",
        "model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
        "model.to(f'cuda:{model.device_ids[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sFOyDc_J9WSd"
      },
      "outputs": [],
      "source": [
        "def hinge_loss(yhat, y):\n",
        "  # The torch loss takes in three arguments so we need to split yhat\n",
        "  # It also expects classes in {+1.0, -1.0} whereas by default we give them in {0, 1}\n",
        "  # Furthermore, if y = 1 it expects the first input to be higher instead of the second,\n",
        "  # so we need to swap yhat[:, 0] and yhat[:, 1]...\n",
        "  torch_loss = torch.nn.MarginRankingLoss(margin=1.0, reduction='none')\n",
        "  y = (y.float() * 2.0) - 1.0\n",
        "  return torch_loss(yhat[:, 1], yhat[:, 0], y)\n",
        "# criterion = hinge_loss\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymFQ6RBs66Qa"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m1Co5fmJBRq9"
      },
      "outputs": [],
      "source": [
        "args['model'] = 'bert'\n",
        "args['n_epochs'] = 3\n",
        "args['lr'] = 1e-5\n",
        "\n",
        "args['robust'] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "NlLOLZnVt-0_",
        "outputId": "7f3ca0b4-7510-4250-eedd-82ac81679755"
      },
      "outputs": [],
      "source": [
        "loss_dict = train(model, criterion, data, args, epoch_offset=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "testloss = loss_dict['test_loss']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.0213, device='cuda:0')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testloss.avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.7442, 1.3884, 1.7484, 2.4439, 2.2785, 3.7817], device='cuda:0')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testloss.avg_group_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WND5PZQRqmZn"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIpr9buaMQ7R"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.scatterplot(model(torch.Tensor(dhs_X).to(device)).detach().cpu().numpy().reshape(-1),\n",
        "                dhs_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Li6qVv6MbjW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.feature_selection import r_regression\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1pcwYTc0zIQ"
      },
      "outputs": [],
      "source": [
        "print(np.sqrt(np.mean((model(torch.Tensor(dhs_X).to(device)).detach().cpu().numpy().reshape(-1)-dhs_y)**2)))\n",
        "print(r2_score(model(torch.Tensor(dhs_X).to(device)).detach().cpu().numpy().reshape(-1), dhs_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xibF4C4LcdW",
        "outputId": "4775683a-8e1f-46db-9ca3-32298120c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.76696825 0.20481146 0.00755109]\n",
            "[4192443.87793695 2166485.01359786  415990.37574294]\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=3)\n",
        "pca.fit(dhs_X)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyEEkJVlLvSj"
      },
      "outputs": [],
      "source": [
        "dhs_X_ = pca.transform(dhs_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umDpPSPxKHv8",
        "outputId": "a57d3f09-00c1-41a5-e162-d41affa2d2fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.46859513, -0.06751622, -0.26220503])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_regression(dhs_X_, dhs_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB-BbuIaeSsD",
        "outputId": "c8da74fc-c7a2-4b25-8bf9-aea03cfc76ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8442211421166622\n",
            "40964.583554132725\n",
            "0.6256822564199208\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dhs_X, dhs_y, test_size=0.2, random_state=42)\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(np.sqrt(np.mean((reg.predict(X_test)-y_test)**2)))\n",
        "print(r2_score(reg.predict(X_test),y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R62SXXQ4gIDl"
      },
      "outputs": [],
      "source": [
        "0.28375245731738574\n",
        "59620.88131970075\n",
        "-1.203185418697995"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hETFtckQ88h-"
      },
      "source": [
        "# Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN0gby6289Y8"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(dhs_X, dhs_y, \n",
        "#                                                     test_size=0.2, \n",
        "#                                                     random_state=42)\n",
        "\n",
        "# regr = AdaBoostRegressor(random_state=0, n_estimators=200)\n",
        "# scores = cross_val_score(regr, dhs_X, dhs_y, cv=5)\n",
        "# print(scores)\n",
        "\n",
        "# regr.fit(dhs_X, dhs_y)\n",
        "\n",
        "# print(r2_score(dhs_y, regr.predict(dhs_X)))\n",
        "# sns.scatterplot(regr.predict(X_test),\n",
        "#                 y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DHS_Modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
